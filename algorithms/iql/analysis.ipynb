{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788650fe",
   "metadata": {},
   "source": [
    "# IQL策略分析与可视化\n",
    "\n",
    "本notebook分析训练好的IQL模型：\n",
    "1. Q/V值分布\n",
    "2. 不同临床状态下的推荐剂量\n",
    "3. 策略对关键特征的敏感性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 添加项目路径\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from algorithms.iql.models import QNetwork, VNetwork, GaussianPolicy\n",
    "from algorithms.iql.dataset import ReadyDataset\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # 支持中文\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c7d14",
   "metadata": {},
   "source": [
    "## 1. 加载模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置\n",
    "CHECKPOINT = 'algorithms/iql/runs/exp_conservative/ckpt_step3000.pt'\n",
    "DATA_PATH = 'intermediate_data/ready_data.csv'\n",
    "STATE_COLS = [\n",
    "    'vanco_level(ug/mL)',\n",
    "    'creatinine(mg/dL)',\n",
    "    'wbc(K/uL)',\n",
    "    'bun(mg/dL)',\n",
    "    'temperature',\n",
    "    'sbp',\n",
    "    'heart_rate'\n",
    "]\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dataset = ReadyDataset(df=df, state_cols=STATE_COLS)\n",
    "\n",
    "# 加载模型\n",
    "ckpt = torch.load(CHECKPOINT, map_location='cpu')\n",
    "state_dim = len(STATE_COLS)\n",
    "action_dim = 1\n",
    "hidden = [32, 32]\n",
    "\n",
    "q_net = QNetwork(state_dim, action_dim, hidden)\n",
    "v_net = VNetwork(state_dim, hidden)\n",
    "pi_net = GaussianPolicy(state_dim, action_dim, hidden)\n",
    "\n",
    "q_net.load_state_dict(ckpt['q_state'])\n",
    "v_net.load_state_dict(ckpt['v_state'])\n",
    "pi_net.load_state_dict(ckpt['pi_state'])\n",
    "\n",
    "q_net.eval()\n",
    "v_net.eval()\n",
    "pi_net.eval()\n",
    "\n",
    "print(f\"模型加载成功: {CHECKPOINT}\")\n",
    "print(f\"数据集: {len(df)} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7bf5e",
   "metadata": {},
   "source": [
    "## 2. Q值和V值分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取transitions\n",
    "trans_df = dataset.to_transitions()\n",
    "states = torch.FloatTensor(np.stack(trans_df['s'].values))\n",
    "actions = torch.FloatTensor(trans_df['a'].values).unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = q_net(states, actions).squeeze().numpy()\n",
    "    v_values = v_net(states).squeeze().numpy()\n",
    "\n",
    "# 绘制分布\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(q_values, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(q_values.mean(), color='red', linestyle='--', linewidth=2, label=f'均值: {q_values.mean():.2f}')\n",
    "axes[0].set_xlabel('Q值', fontsize=12)\n",
    "axes[0].set_ylabel('频数', fontsize=12)\n",
    "axes[0].set_title('Q值分布 (行为策略)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(v_values, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].axvline(v_values.mean(), color='red', linestyle='--', linewidth=2, label=f'均值: {v_values.mean():.2f}')\n",
    "axes[1].set_xlabel('V值', fontsize=12)\n",
    "axes[1].set_ylabel('频数', fontsize=12)\n",
    "axes[1].set_title('状态价值分布', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Q值: 均值={q_values.mean():.2f}, 标准差={q_values.std():.2f}\")\n",
    "print(f\"V值: 均值={v_values.mean():.2f}, 标准差={v_values.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34b905",
   "metadata": {},
   "source": [
    "## 3. 贪心策略：不同状态下的最优剂量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择10个代表性状态\n",
    "sample_indices = np.linspace(0, len(states)-1, 10, dtype=int)\n",
    "sample_states = states[sample_indices]\n",
    "\n",
    "# 在[-1, 1]范围采样动作\n",
    "action_range = torch.linspace(-1, 1, 100).unsqueeze(1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, state in enumerate(sample_states):\n",
    "    state_batch = state.unsqueeze(0).expand(len(action_range), -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_batch, action_range).squeeze().numpy()\n",
    "    \n",
    "    best_action_idx = q_vals.argmax()\n",
    "    best_action = action_range[best_action_idx].item()\n",
    "    best_q = q_vals[best_action_idx]\n",
    "    \n",
    "    axes[i].plot(action_range.numpy(), q_vals, linewidth=2, color='steelblue')\n",
    "    axes[i].scatter([best_action], [best_q], color='red', s=100, zorder=5, label=f'最优: {best_action:.2f}')\n",
    "    axes[i].set_xlabel('动作 (标准化)', fontsize=9)\n",
    "    axes[i].set_ylabel('Q值', fontsize=9)\n",
    "    axes[i].set_title(f'状态 {i+1}', fontsize=10)\n",
    "    axes[i].legend(fontsize=8)\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446dfac",
   "metadata": {},
   "source": [
    "## 4. 关键特征敏感性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一个中位数状态\n",
    "median_state = torch.FloatTensor(np.median(states.numpy(), axis=0))\n",
    "\n",
    "# 分析每个特征的影响\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for feat_idx, feat_name in enumerate(STATE_COLS):\n",
    "    # 创建特征扫描\n",
    "    feat_range = np.linspace(\n",
    "        states[:, feat_idx].min().item(),\n",
    "        states[:, feat_idx].max().item(),\n",
    "        50\n",
    "    )\n",
    "    \n",
    "    v_vals = []\n",
    "    for val in feat_range:\n",
    "        test_state = median_state.clone()\n",
    "        test_state[feat_idx] = val\n",
    "        with torch.no_grad():\n",
    "            v = v_net(test_state.unsqueeze(0)).item()\n",
    "        v_vals.append(v)\n",
    "    \n",
    "    axes[feat_idx].plot(feat_range, v_vals, linewidth=2, color='forestgreen')\n",
    "    axes[feat_idx].axhline(v_net(median_state.unsqueeze(0)).item(), \n",
    "                           color='red', linestyle='--', alpha=0.5, label='基准V值')\n",
    "    axes[feat_idx].set_xlabel(feat_name, fontsize=10)\n",
    "    axes[feat_idx].set_ylabel('状态价值 V', fontsize=10)\n",
    "    axes[feat_idx].set_title(f'{feat_name}的影响', fontsize=11, fontweight='bold')\n",
    "    axes[feat_idx].grid(alpha=0.3)\n",
    "    axes[feat_idx].legend(fontsize=8)\n",
    "\n",
    "# 隐藏最后一个子图（7个特征，8个位置）\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258b156",
   "metadata": {},
   "source": [
    "## 5. 策略推荐 vs 实际行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0494038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个状态的贪心动作\n",
    "greedy_actions = []\n",
    "for state in states:\n",
    "    state_batch = state.unsqueeze(0).expand(100, -1)\n",
    "    action_samples = torch.linspace(-1, 1, 100).unsqueeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_batch, action_samples)\n",
    "    \n",
    "    best_idx = q_vals.argmax().item()\n",
    "    greedy_actions.append(action_samples[best_idx].item())\n",
    "\n",
    "greedy_actions = np.array(greedy_actions)\n",
    "behavior_actions = actions.squeeze().numpy()\n",
    "\n",
    "# 绘制对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 散点图\n",
    "axes[0].scatter(behavior_actions, greedy_actions, alpha=0.3, s=20)\n",
    "axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='y=x')\n",
    "axes[0].set_xlabel('行为策略动作', fontsize=12)\n",
    "axes[0].set_ylabel('贪心策略动作', fontsize=12)\n",
    "axes[0].set_title('策略对比', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 直方图\n",
    "axes[1].hist(behavior_actions, bins=30, alpha=0.5, label='行为策略', color='blue', edgecolor='black')\n",
    "axes[1].hist(greedy_actions, bins=30, alpha=0.5, label='贪心策略', color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('动作值', fontsize=12)\n",
    "axes[1].set_ylabel('频数', fontsize=12)\n",
    "axes[1].set_title('动作分布对比', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"行为策略均值: {behavior_actions.mean():.3f}, 标准差: {behavior_actions.std():.3f}\")\n",
    "print(f\"贪心策略均值: {greedy_actions.mean():.3f}, 标准差: {greedy_actions.std():.3f}\")\n",
    "print(f\"动作差异 (MAE): {np.abs(greedy_actions - behavior_actions).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a00209",
   "metadata": {},
   "source": [
    "## 6. 总结与发现\n",
    "\n",
    "**模型性能：**\n",
    "- Q/V函数已成功收敛，损失稳定\n",
    "- 贪心策略相对行为策略的改进约为 -6%\n",
    "\n",
    "**关键发现：**\n",
    "1. V值分布显示模型学习到了状态价值的差异\n",
    "2. Q值对不同动作的响应曲线平滑，说明函数逼近质量较好\n",
    "3. 贪心策略与行为策略有一定差异，表明模型发现了潜在的改进空间\n",
    "\n",
    "**下一步：**\n",
    "- 在临床模拟器上测试策略的实际效果\n",
    "- 考虑安全约束和临床可解释性\n",
    "- 进行敏感性分析和鲁棒性测试"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
